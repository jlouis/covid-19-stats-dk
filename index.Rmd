---
title: "Covid-19 Stats for Denmark"
date: 2022-03-01
output:
  html_document:
    toc: true
---

## Strategy

Data processing is best handled by writing a program which processes the data, rather than using interactive
methods by hand. The reason is underlying data changes dynamically over time, but the *program* stays the
same. This means we can simply re-run the program whenever data changes and get new visualizations.

This visualization uses *R* which is a language mostly for statistical processing. It also has a very strong
visualization side, and this is used by SSI in Denmark for their reporting. The original compartmental models
by SSI for Covid-19, used for predictions, also used *R*.

For our work here, *R* is a good choice because it has lots of tooling for working with tables of data. And
SSI provide their data in tabular format.

## Initial setup

To process data sets from SSI, we'll need some packages for doing so. We'll
load `tidyverse`, and `lubridate`.

The `lubridate` package empowers our *R* system with better handling of dates and timestamps. The `tidyverse`
package loads a slew of other packages for *R*. These empower our system to be more like a *functional*
programming language, which I tend to like working in.

You can grab SSI's data from [their homepage](https://covid19.ssi.dk/overvagningsdata/download-fil-med-overvaagningdata)

```{r setup, include = TRUE, echo = TRUE, message = FALSE}
library(tidyverse)
library(lubridate)
```

SSI's data is in a zip file. We unpack this zip file into a directory of the
general form `YYYY-MM-DD`. Lets set up a name for the current directory we
want to grab data from

```{r constants}
date <- "2022-03-01"
```

## Ingesting data

The overall strategy follows a 3-step process:

* *Parse* the incoming data, providing a proper interpretation of what it means.
* *Transform* the data, making it amenable for data-visualization.
* *Visualize* the data, aiding the understanding of patterns, evaluations, and structure.

SSIs data are in "CSV" files. The CSV format isn't generally well specified, and the variant
used by SSI is one in which the `;` character is used as a separator. This is
relatively common in countries using the `,` as a separator in numbers. R's `readr` package,
loaded by `tidyverse`, have a way to handle this particular CSV variant.

Furthermore, the files contain Danish characters. They are encoded in the ISO8859-1(5)
format, so we'll have to do something about this. The obvious way is to turn the
encoding into UTF-8, which is the modern encoding of Unicode.

We'll capture this conversion into a function `read_data`. It uses
[iconv](https://en.wikipedia.org/wiki/Iconv) to convert between encodings. The function
also allows us to designate the types of the columns in the file we are trying to read.

```{r read-function}
read_data <- function(f, col_types) {
    raw <- readBin(f, "raw", n = file.size(f))
    x_utf8 <- iconv(list(raw), from = "ISO-8859-1", to = "UTF-8", toRaw = TRUE)[[1]]

    # CSV variant 2 uses ; as the separator, among other adaptations
    res <- read_csv2(x_utf8, col_types=col_types)
    return(res)
}
```

### Daily key figures

SSI provides a file with key figures. This file is updated 5 days a week. Since we already have
set a target date, we can compute the file with key figures in it by substitution

```{r}
file <- sprintf("./data/%s/Regionalt_DB/03_bekraeftede_tilfaelde_doede_indlagte_pr_dag_pr_koen.csv", date)
```

We can read this file via our function. It will return a so-called `tibble` which is a table, or data frame,
of the data in the file. We also provide initial types for each of the columns, because CSV data is untyped.

We would like to add type information to columns because it simplifies our later plotting. Many tools in R
know how to vary it's handling, based on the type.

In the following we use the following types:

* `col_date()` is a date, with no particular timestamp.
* `col_double()` is a floating point number. This allows parses of numbers in general, while still handling integers adequately.
* `col_factor()` is used the define factor values. These are small (discrete) sets of values, in this case the different Regions of Denmark.

```{r}
data <- read_data(file, col_types=cols('Region' = col_factor(),
                                       'Prøvetagningsdato' = col_date(),
                                       'Køn' = col_factor(),
                                       'Bekræftede tilfælde i alt' = col_double(),
                                       'Døde' = col_double(),
                                       'Indlæggelser' = col_double()
                                      ))
data
```

We certainly don't want to work with danish-named columns. Rather, we want small
descriptive names which are easy to type. We send our dataset through a `rename`
pass for this.

And while we are here, lets also limit ourselves to the columns we are interested
in. There are some cumulative columns in the dataset, but we are not going to be
interested in those. So we select the columns we care about.

```{r key_rename}
data <- data %>% rename(region = 'Region',
                        date = 'Prøvetagningsdato',
                        gender = 'Køn', # This might be wrong, because it could also be 'sex'. However, we are going to aggregate it
                        cases = 'Bekræftede tilfælde i alt',
                        deaths = 'Døde',
                        hospitalizations = 'Indlæggelser')

# Cut the cumulative columns away
data <- data %>% select(region, date, gender, cases, deaths, hospitalizations)
data
```

Provide a low cut-off. We are interested in the recent period with Omicron, rather
than the full duration of Covid-19.

```{r}
lo_cutoff <- '2021-09-01'
```

The high end of that cutoff, we set to two days before the maximal date in our data set

```{r}
hi_cutoff <- max(data$date) - days(2)
```

Rationale: News outlets generally report a cumulative count increase . The
actual date of testing is often lagging by 24h to 48h. We are interested in
getting the right test-date, even if it means we have a 2 days lag. Hence, we
assume that recent data are still "forming".

A typical example is that the count for the current date is very close to 0, as
no tests have come in yet. In 2-3 days time, this number rises to the right
value, as data is retroactively updated.

Using the cumulative count means that the test date is being moved to some other
date, and this will skew any kind of trend-line attempt.

Filtering the data set is easy:

```{r summary}
data <- data %>% filter(date >= lo_cutoff, date < hi_cutoff)

# Put this into the summary
summary <- data
summary
```

### Weekly key figures split by age

As in the case with the general data, we can define the file of interest

```{r}
file <- sprintf("./data/%s/Regionalt_DB/18_fnkt_alder_uge_testede_positive_nyindlagte.csv", date)
```

We can reuse our data reader from before.

```{r}
data <- read_data(file, col_types=cols('Uge' = col_character(), # This will need fixing later
                                       'Aldersgruppe' = col_factor(),
                                       'Testede pr. 100.000 borgere' = col_double(),
                                       'Positive pr. 100.000 borgere' = col_double(),
                                       'Nyindlagte pr. 100.000 borgere' = col_double(),
                                       'Antal testede' = col_double(),
                                       'Antal positive' = col_double()
                                      ))
```

Rename the columns to something more amenable to further handling

```{r}
data <- data %>% rename(week = 'Uge',
                        ageGroup = 'Aldersgruppe',
                        tested_100k = 'Testede pr. 100.000 borgere',
                        cases_100k = 'Positive pr. 100.000 borgere',
                        hospitalized_100k = 'Nyindlagte pr. 100.000 borgere',
                        tested = 'Antal testede',
                        cases = 'Antal positive')
data
```

Take a look at the `week` column. It reports a year, and a week number. We'll need to handle this,
by converting this into a date. We'll do that by defining a fixup function. If we have a string such
as `2020-W04`, it means the 4th week of 2020. To get a date, we need a day inside the week to use
as the day-of-week. So we turn the string into `2020-W04-1`, then send it through `strptime()`,
a general time-parsing function.

```{r}
fixup_week <- function(s) {
    # For strptime to do its work correctly, it must know a day of the week. Here, we
    # use the 1st day of the week as a anchoring point, but in principle any day could
    # have been used.
    x <- sprintf("%s-1", s)
    res <- strptime(x, format="%Y-W%W-%u")
    return(as.POSIXct(res))
}
```

With the function defined, we can `mutate` our data set by the fixup, setting each value in the table
to be the fixup'ed value. While here, also cut off the older values since we are mostly interested
in the recent data.

```{r, warning = FALSE}
lo_cutoff <- "2021-11-01"

# Apply this to the data set
data <- data %>% mutate(week = fixup_week(week))
data <- data %>% filter(week >= lo_cutoff)
```

Finally, lets use a good name for this dataset (tibble):

```{r}
weekly <- data
```

### Data for muncipalities

The munciplality data is in a seperate file:

```{r}
file <- sprintf("./data/%s/Kommunalt_DB/10_Kommune_kort.csv", date)
```

This needs to be read, like the other data sets

```{r}
data <- read_data(file, col_types=cols('Kommune' = col_double(), # This will need fixing later
                                       'Kommunenavn' = col_factor(),
                                       'Bekræftede tilfælde i alt' = col_double(),
                                       'Incidens' = col_double(),
                                       'Bekræftede tilfælde i alt de seneste 7 dage' = col_double(),
                                       'Incidens de seneste 7 dage' = col_double(),
                                       'Antal testede personer' = col_double(),
                                       'Testestede pr. 100.000' = col_double(),
                                       'Testede de seneste 7 dage' = col_double(),
                                       'Testede pr. 100.000 de seneste 7 dage' = col_double(),
                                       'Antal borgere' = col_double()
                                      ))
```

And like before, this needs renaming

```{r}
data <- data %>% rename(lau1 = 'Kommune',
                        name = 'Kommunenavn',
                        incidence = 'Incidens de seneste 7 dage',
                        cases = 'Bekræftede tilfælde i alt de seneste 7 dage',
                        tested = 'Antal testede personer',
                        tested_100k = 'Testestede pr. 100.000',
                        tested_1w = 'Testede de seneste 7 dage',
                        tested_100k_1w = 'Testede pr. 100.000 de seneste 7 dage',
                        population = 'Antal borgere',
                        )
data
```

```{r}
muncipalities <- data
```

## Country-wide data

Let's analyze the country-wide data set. Because data is given by each region,
we must first summarize the data by summing the data from each region per day. This
means we group by the `date` and then sum

```{r}
# Group by date, so we can get rid of region/gender
country <- summary %>%
    group_by(date) %>%
    summarize(cases = sum(cases),
              deaths = sum(deaths),
              hospitalizations = sum(hospitalizations))
```

We also want a lag-column. That is, for each date, we want the count from the
day preceding it. This allows us to compute the change for each day.

We also add a new column `weekday`, which computes the day of the week. This
allows us to visualize weekly periodic fluctuations in reporting. We might not
know the cause of such noise in the data set, but it is still important to
show patterns when they occur.

```{r}
# Create a lag column for cases
country <- country %>% mutate(previous_cases = lag(cases, order_by=date),
                              weekday = lubridate::wday(date, label=TRUE))
```

### Country-wide visualizations

At this point, we have readied our data, and can start plotting. First, lets plot
the number of cases we have in Denmark, and supply a trend-line for the cases

(*Aside:* We use ggplot2 to visualize. ggplot uses a *grammar* of graphical layers
as its plotting strategy. For instance `p + geom_point()` is a form of "addition"
where `p` is a plot and `geom_point()` is an x-y scatterplot of points. So the `+`
means "add another layer on top of our plot."

The other key idea is that of the `aes(..)` function. This is an *aesthetic*-mapping.
For example, the expression `aes(x=date, y=cases)` maps the `date` column to the
x-axis, and the `cases` column to the y-axis. Likewise, `aes(color = weekday)` maps a
tables `weekday` column into a "color dimension.")

The trend-line is computed by [Local Regression](https://en.wikipedia.org/wiki/Local_regression),
or LOESS (LOcally Estimated Scatterplot Smoothing). This is a generalization of a moving
average over the data. The `span = .3` parameter sets the bandwidth to 30% of the data.

LOESS is a trade-off where we throw computational power at the problem to obtain good
smoothed trend lines. It would be impossible to do by hand, but we have access to
modern computers with ample computational power.

```{r country-cases}
plot_country <- function(p, title, caption) {
    p + geom_point(aes(color = weekday), size=2) +
        geom_smooth(method = 'loess', span = .3) +
        scale_colour_brewer(palette="Set2") +
        labs(title = title, caption = caption)
}

p <- ggplot(country, aes(x=date, y=cases))
plot_country(p, "Trendline for Covid-19 cases in Denmark", "Source: ssi.dk")
```

We can use the same plot function to handle hospitalizations in Denmark

```{r country-hospitalizations}
p <- ggplot(country, aes(x=date, y=hospitalizations))
plot_country(p, "Trendline for Covid-19: new hospitalizations in Denmark", "Source: ssi.dk")
```

And likewise for deaths. Since deaths tend to have a larger lag before reporting, we don't
report the last 6 days

```{r country-deaths}
hi_cutoff_deaths <- max(country$date) - days(6)
p <- ggplot(country %>% filter(date < hi_cutoff_deaths), aes(x=date, y=deaths))
plot_country(p, "Trendline for Covid-19: new deaths in Denmark", "Source: ssi.dk")
```

The next plot computes the *change* from the day before to the next day. A positive
number means "There were this many more cases today." A negative number means "There
were this fewer cases today." This shows how data is getting more noisy once Omicron
takes over and more tests are being made.

As we shall see, this noise mostly comes from a single region in Denmark.

```{r country-lagged}
# Lagged data computes the difference to the day before.
plot_country_lag <- function(p, title, caption) {
    p + geom_point(size=1.5) +
        scale_colour_brewer(palette="Set2") +
        labs(title = title, caption = caption)
}

p <- ggplot(country, aes(x=date, y=(cases - previous_cases), color=weekday))
plot_country_lag(p, "As Omicron took over, noise entered data in Denmark", "Source: ssi.dk")
```

Another way of plotting this is to plot the number of cases today on one axis, and the previous
cases from the day before on another axis.

```{r country-lagged-scatter}
p <- ggplot(country, aes(x=previous_cases, y=cases, color=weekday))
plot_country_lag(p, "Delta change on cases", "Source: ssi.dk") +
  geom_abline(slope=1, color='gray', linetype=2)
```

The fact Monday and Sunday are always above the dashed line means they always provide an
increase in case count compared to other days. The days where these numbers tend to be
entered into the database are Monday and Tuesday.

Because there are some clear day-of-week structure in the data set, it's worth
looking a bit into this. In particular:

* `Monday` tends to have large positive change.
* `Wednesday` tend to have a smaller decrease.

```{r country-weekday}
plot_day_of_week <- function(p, title, caption) {
    p + scale_colour_brewer(palette="Set2") +
        stat_boxplot(aes(color=weekday), alpha=0.3) +
        labs(title = title, caption = caption)
}

p <- ggplot(country, aes(x=weekday, y=(cases - previous_cases)))
plot_day_of_week(p, "Some days of the week has clear over/under-representation", "Source: ssi.dk")
```

## Regional data

Compute the aggregate over regions, rather than for the whole country

```{r regional-handling}
regions <- summary %>%
  group_by(date, region) %>%
  summarize(cases = sum(cases), deaths = sum(deaths), hospitalizations = sum(hospitalizations))

regions <- regions %>% mutate(weekday = lubridate::wday(date, label=TRUE), order_by=date)
```

### Regional visualizations

To visualize the regional data, we use a facet grid, splitting on region. As we see, there are a lot
of noise data generated by "Hovedstaden" (corresponding to the general area around Copenhagen). This
region is generating most cases, by far.

```{r regional-cases}
plot_regions <- function(p, title, subtitle) {
    p + geom_point(aes(color=region), size=.5) +
        facet_grid(rows=vars(region)) +
        scale_colour_brewer(palette="Set2") +
        labs(title = title,
             subtitle = subtitle,
             caption = "Source: ssi.dk",
             x = "Date",
             y = "Detected Cases")
}

p <- ggplot(regions, aes(x=date, y=cases))
plot_regions(p, "Regional split of cases, DK",
                "These numbers are somewhat dodgy because of changes in test strategy")
```

We can also visualize regional new hospitalizations and deaths:

```{r regional-hospitalizations}
p <- ggplot(regions, aes(x=date, y=hospitalizations))
p + geom_point(aes(color=region), size=.5) +
    facet_grid(rows=vars(region)) +
    scale_colour_brewer(palette="Set2") +
    labs(title = "Regional split of Hospitalizations, DK",
         subtitle = "Data provides an upper bound on the actual hospitalization count",
         caption = "Source: ssi.dk",
         x = "Date",
         y = "New Hospitalizations")
```

```{r regional-deaths}
p <- ggplot(regions, aes(x=date, y=deaths))
p + geom_point(aes(color=region), size=.5) +
    facet_grid(rows=vars(region)) +
    scale_colour_brewer(palette="Set2") +
    labs(title = "Regional split of Deaths, DK",
         subtitle = "Data provides an upper bound on the actual death count",
         caption = "Source: ssi.dk",
         x = "Date",
         y = "New Deaths")
```

## Weekly data

As with the case for daily key figures, we can also use the weekly key figures. An important point of
this data set is that the `ageGroup` are not a continous variable, but a discrete set of groups, non-uniformly
spaced in between:

```{r agegroup-values}
levels(factor(weekly$ageGroup))
```

Why this is so is a good question. My bet is that it captures schooling groups among the youngest, splits
adults into younger and older adults pivoting on 40, and handles the elderly in two large groups as well. However,
it also means direct comparison between groups is somewhat dangerous.

```{r agegroup-width, include=FALSE}
ageGroup_width <- function(ag) {
    return(case_when(
        ag == '0-2' ~ 2 - 0,
        ag == '3-5' ~  5 - 3,
        ag == '6-11'  ~ 11 - 6,
        ag == '12-15' ~ 15 - 12,
        ag == '16-19' ~ 19 - 16,
        ag == '20-39' ~ 39 - 20,
        ag == '40-64' ~ 64 - 40,
        ag == '65-79' ~ 79 - 65,
        ag == '80+' ~ 100 - 80,
        TRUE ~ 0
    ))
}

weekly <- weekly %>% mutate(ageBand = ageGroup_width(ageGroup))
weekly$ageBand
```

### Visualizations

We can provide a simple raster plot, where `x` represents the week, `y`
represents the (discrete) age group and the fill represents the count we are
interested in. In principle, the age groups have different sizes, but
we can adjust for the size difference in groups by looking at "x per 100.000".

```{r age-split-cases}
plot_age_cases <- function(p, title, subtitle) {
    p + geom_tile() +
        scale_fill_continuous(type = "viridis") +
        labs(title = title, subtitle = subtitle, caption = "Source: ssi.dk", x="Week", y = "Age Group")
}
```

The above function allows us to plot cases per age group, and tested per age group

```{r age-group-cases}
p <- ggplot(weekly, aes(x=week, y=ageGroup, fill=cases_100k, height=0.8))
plot_age_cases(p, "Weekly cases split per age group, DK", "Schools were closed on 15th Dec 2021, reopened in Jan 2022")
```

```{r age-group-tests}
p <- ggplot(weekly, aes(x=week, y=ageGroup, fill=tested_100k, height=0.8))
plot_age_cases(p, "Weekly tests split per age group, DK", "Schools were closed on 15th Dec 2021, reopened in Jan 2022")
```

## Muncipality data

To compute incidence, we first compute a z-value as the difference from the mean. This sets
us up for a label which are either above or below the mean value.

```{r preprocess-muncipality-data}
muncipalities$incidence_z <- round(muncipalities$incidence - mean(muncipalities$incidence), 2)
muncipalities$incidence_type <- ifelse(muncipalities$incidence_z < 0, "Below mean", "Above mean")
```

Reorder the muncipality factor levels according to the incidence score, then plot a diverging bars
plot for the data.

```{r muncipality-incidence-overview, fig.height=12}
ggplot(muncipalities %>% mutate(name = fct_reorder(name, incidence_z)), aes(x=name, y=incidence)) +
  geom_bar(stat='identity', aes(fill=incidence_type), width=.5)  +
  scale_colour_brewer(palette="Set2") +
  geom_hline(yintercept=mean(muncipalities$incidence), linetype=2) +
  labs(subtitle="Incidence",
       title= "Diverging Bars",
       fill = "Incidence",
       x = "Name",
       y = "Incidence over the last 7 days",
       caption = "Source: ssi.dk") +
  coord_flip()
```

More computation allows us to do the same for cases per test

```{r preprocess-muncipality-data-2}
muncipalities$rate <- muncipalities$cases / muncipalities$tested_1w
muncipalities$rate_z <- round(muncipalities$rate - mean(muncipalities$rate), 5)
muncipalities$rate_type <- ifelse(muncipalities$rate_z < 0, "Below mean", "Above mean")
muncipalities$rate
```

```{r muncipality-rate-overview, fig.height=12}
ggplot(muncipalities %>% mutate(name = fct_reorder(name, rate)), aes(x=name, y=rate)) +
  geom_bar(stat='identity', aes(fill=rate_type), width=.5)  +
  scale_colour_brewer(palette="Set2") +
  geom_hline(yintercept=mean(muncipalities$rate), linetype=2) +
  labs(title= "Diverging Bars Plot, DK",
       subtitle = "Cases/Tested ratio over the last 7 days",
       fill = "rate",
       x = "Name",
       y = "Rate",
       caption = "Source: ssi.dk") +
  coord_flip()
```

### Analyzing infections over population counts

Read in Popcount. These can be obtained by Danmarks Statistik, by query on `FOLK1AM` in their
data bank:

```{r read-popcount}
read_data_2 <- function(f) {
    raw <- readBin(f, "raw", n = file.size(f))
    x_utf8 <- iconv(list(raw), from = "ISO-8859-1", to = "UTF-8", toRaw = TRUE)[[1]]

    # CSV variant 2 uses ; as the separator, among other adaptations
    res <- readr::read_csv2(x_utf8, col_names=FALSE)
    return(res)
}

popcount <- read_data_2("./data/DS/2022121185617360647690FOLK1AM68563293012.csv")
popcount <- popcount %>% rename(name = 3, popcount = 4)
popcount <- popcount %>% select(name, popcount)
popcount
```

Join the tables, which will happen on the muncipality name. I would have liked a better
key for the join, but this will do:

```{r}
muncipalities <- muncipalities %>% left_join(popcount)
muncipalities %>% select(name, incidence, popcount)
```

Now, test correlation via Pearson:

```{r correlation-popcount-incidence}
plot(muncipalities$incidence, muncipalities$popcount)
cor.test(muncipalities$incidence, muncipalities$popcount)
```

Clearly, this argues there are no strong connection between population count,
and the incidence in a given muncipality.

Analyzing the rate shows there's a connection between it and incidence, but
this is more expected:
```{r correlation-incidence-rate}
pairs(muncipalities %>% select(incidence,popcount,rate))
cor.test(muncipalities$incidence, muncipalities$rate)
```

Summarize:

```{r muncipality-summary}
summary(muncipalities$rate)
d <- density(muncipalities$rate)
plot(d)
```
